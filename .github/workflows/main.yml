name: Crawler Ao Vivo e Resumo Periodico

on:
  schedule:
    # Cron 1: "Ao vivo" (O GitHub limita o mínimo a cada 5-10 min na prática)
    - cron: '*/5 * * * *' 
    # Cron 2: A cada 6 horas (Exatamente às 00h, 06h, 12h e 18h)
    - cron: '0 */6 * * *'
  workflow_dispatch: # Permite rodar manualmente pelo painel do GitHub

jobs:
  # --- JOB DO CRAWLER (AO VIVO) ---
  crawler_ao_vivo:
    # Só executa se for o cron de 5 min ou disparo manual
    if: github.event.schedule == '*/5 * * * *' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout do Código
        uses: actions/checkout@v4

      - name: Configurar Ambiente (Ex: Python)
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Instalar Dependencias
        run: pip install -r requirements.txt

      - name: Executar Crawler
        run: python crawler.py --mode live

  # --- JOB DO RESTO (A CADA 6 HORAS) ---
  processamento_6h:
    # Só executa se for o cron de 6 horas ou disparo manual
    if: github.event.schedule == '0 */6 * * *' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout do Código
        uses: actions/checkout@v4

      - name: Executar Processamento Pesado
        run: |
          echo "Iniciando limpeza e análise de dados das últimas 6 horas..."
          python main.py --mode full

